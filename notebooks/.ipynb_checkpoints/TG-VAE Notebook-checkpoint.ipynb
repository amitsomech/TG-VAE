{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.table2graph import create_corpus, shuffle_vocabulary, build_graph_edges, build_node_features\n",
    "from utils.vectorize import generate_table_vectors\n",
    "from utils.evaluation import evaluate_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import (negative_sampling, remove_self_loops,\n",
    "                                   add_self_loops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Dataset loading & Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"./datasets/benchmarks/rossman200.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pickle.load(open(dataset_path,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 17\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset),len(dataset[0][\"table\"].columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF = {\n",
    "    \"add_attr\":True,\n",
    "    \"shuffle_vocab\": True,\n",
    "    \"add_columns\":False,\n",
    "    \"vector_size\":16,\n",
    "    \"row_edges_sample\":1.0,\n",
    "    \"column_edges_sample\":0.1,\n",
    "    \"epoch_num\":20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific model settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF[\"n2v_walk_length\"] = 20\n",
    "CONF[\"n2v_context_size\"] = 10\n",
    "CONF[\"n2v_walks_per_node\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build Table graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tables, vocabulary, cell_dict, reversed_dictionary = corpus_tuple = create_corpus(dataset,include_attr=CONF[\"add_attr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONF[\"shuffle_vocab\"] == True:\n",
    "    shuffled_vocab = shuffle_vocabulary(vocabulary)\n",
    "else:\n",
    "    shuffled_vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = build_node_features(vocabulary)\n",
    "row_edges_index, row_edges_weights = build_graph_edges(tokenized_tables,s_vocab=shuffled_vocab,sample_frac=CONF[\"row_edges_sample\"],columns=False)\n",
    "col_edges_index, col_edges_weights = build_graph_edges(tokenized_tables,s_vocab=shuffled_vocab,sample_frac=CONF[\"column_edges_sample\"],columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_edges_index, all_row_edges_weights = build_graph_edges(tokenized_tables,s_vocab=shuffled_vocab,sample_frac=1.0,columns=False)\n",
    "all_col_edges_index, all_col_edges_weights = build_graph_edges(tokenized_tables,s_vocab=shuffled_vocab,sample_frac=1.0,columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_edges= torch.cat((all_row_edges_index,all_col_edges_index),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/benchmarks/rossman200.pickle\n"
     ]
    }
   ],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54596 1857682 7034472 8892154\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes),all_row_edges_index.shape[1],all_col_edges_index.shape[1],all_possible_edges.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = torch.cat((row_edges_index,col_edges_index),dim=1)\n",
    "weights= torch.cat((row_edges_weights,col_edges_weights),dim=0)\n",
    "graph_data = Data(x=nodes,edge_index=edges,edge_attr=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def get_column_by_node(node,r_dict):\n",
    "    return r_dict[node][0]\n",
    "\n",
    "def get_column_ids(vocab,s_vocab=None):\n",
    "    if s_vocab is not None:\n",
    "        node_columns = [get_column_by_node(n,reversed_dictionary) for n in s_vocab]\n",
    "    else:\n",
    "        node_columns = [get_column_by_node(n,reversed_dictionary) for n in range(vocab)]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    node_columns_ids = le.fit_transform(node_columns)\n",
    "    return torch.tensor(node_columns_ids,dtype=torch.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=get_column_ids(vocabulary,s_vocab=shuffled_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data.cols = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ) Run Table Auto-Encoder Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, train_pos_edge_index = split_edges(graph_data)\n",
    "x, train_pos_edge_index = nodes,edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "MAX_LOGVAR = 10\n",
    "\n",
    "class TVGAE(GAE):\n",
    "    r\"\"\"The Variational Graph Auto-Encoder model from the\n",
    "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
    "    paper.\n",
    "\n",
    "    Args:\n",
    "        encoder (Module): The encoder module to compute :math:`\\mu` and\n",
    "            :math:`\\log\\sigma^2`.\n",
    "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
    "            will default to the\n",
    "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
    "            (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super(TVGAE, self).__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logvar) * torch.exp(logvar)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        self.__rmu__, self.__rlogvar__,self.__cmu__, self.__clogvar__ = self.encoder(*args, **kwargs)\n",
    "        self.__rlogvar__ = self.__rlogvar__.clamp(max=MAX_LOGVAR)\n",
    "        self.__clogvar__ = self.__clogvar__.clamp(max=MAX_LOGVAR)\n",
    "        zr = self.reparametrize(self.__rmu__, self.__rlogvar__)\n",
    "        zc = self.reparametrize(self.__cmu__, self.__clogvar__)\n",
    "        z=torch.cat((zr,zc),0)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def kl_loss(self):\n",
    "\n",
    "        rmu = self.__rmu__ \n",
    "        rlogvar = self.__rlogvar__ \n",
    "\n",
    "        cmu = self.__cmu__ \n",
    "        clogvar = self.__clogvar__ \n",
    "        \n",
    "        rkl= -0.5 * torch.mean(\n",
    "            torch.sum(1 + rlogvar - rmu**2 - rlogvar.exp(), dim=1))\n",
    "        ckl= -0.5 * torch.mean(\n",
    "            torch.sum(1 + clogvar - rmu**2 - clogvar.exp(), dim=1))\n",
    "        return(rkl,ckl)\n",
    "\n",
    "        \n",
    "    def recon_loss(self,z, pos_edge_index,all_possible_edges):\n",
    "        EPS = 1e-15\n",
    "        MAX_LOGVAR = 10\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            model.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n",
    "\n",
    "        # Do not include self-loops in negative samples\n",
    "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n",
    "        pos_edge_index, _ = add_self_loops(pos_edge_index)\n",
    "\n",
    "        neg_edge_index = negative_sampling(all_possible_edges, z.size(0))\n",
    "        neg_loss = -torch.log(1 -\n",
    "                              model.decoder(z, neg_edge_index, sigmoid=True) +\n",
    "                              EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_rows = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv_cols = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        \n",
    "        self.conv_rmu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_rlogvar = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "        self.conv_cmu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_clogvar = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "             \n",
    "        \n",
    "    def forward(self, x, row_edge_index,col_edge_index):\n",
    "        xr = F.relu(self.conv_rows(x, row_edge_index))\n",
    "        xc = F.relu(self.conv_cols(x, col_edge_index))\n",
    "        return self.conv_rmu(xr, row_edge_index),\\\n",
    "            self.conv_rlogvar(xr, row_edge_index),\\\n",
    "            self.conv_cmu(xc, col_edge_index),\\\n",
    "            self.conv_clogvar(xc, col_edge_index)\n",
    "    \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=CONF[\"vector_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(graph_data.num_features, channels)\n",
    "model = TVGAE(enc)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,optimizer,x, row_edges,col_edges):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, row_edges,col_edges)\n",
    "    mid = int(len(z)/2)\n",
    "    zr=z[:mid]\n",
    "    zc=z[mid:]\n",
    "    \n",
    "    #recon loss:\n",
    "    #row_loss = model.recon_loss(zr,row_edges,all_possible_edges)\n",
    "    #col_loss = model.recon_loss(zc,col_edges,all_possible_edges)\n",
    "    #loss = row_loss+col_loss\n",
    "    \n",
    "    rkl,ckl = model.kl_loss()\n",
    "    loss = rkl+ckl\n",
    "    \n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #return loss,row_loss,col_loss\n",
    "    return loss,rkl,ckl\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (tensor(14.3631, grad_fn=<AddBackward0>), tensor(10.2363, grad_fn=<AddBackward0>), tensor(4.1268, grad_fn=<AddBackward0>))\n",
      "1 (tensor(9.3202, grad_fn=<AddBackward0>), tensor(5.5652, grad_fn=<AddBackward0>), tensor(3.7550, grad_fn=<AddBackward0>))\n",
      "2 (tensor(6.9965, grad_fn=<AddBackward0>), tensor(3.5404, grad_fn=<AddBackward0>), tensor(3.4561, grad_fn=<AddBackward0>))\n",
      "3 (tensor(6.0272, grad_fn=<AddBackward0>), tensor(3.0189, grad_fn=<AddBackward0>), tensor(3.0083, grad_fn=<AddBackward0>))\n",
      "4 (tensor(5.5722, grad_fn=<AddBackward0>), tensor(2.7613, grad_fn=<AddBackward0>), tensor(2.8108, grad_fn=<AddBackward0>))\n",
      "5 (tensor(5.2403, grad_fn=<AddBackward0>), tensor(2.6031, grad_fn=<AddBackward0>), tensor(2.6372, grad_fn=<AddBackward0>))\n",
      "6 (tensor(4.9703, grad_fn=<AddBackward0>), tensor(2.4791, grad_fn=<AddBackward0>), tensor(2.4912, grad_fn=<AddBackward0>))\n",
      "7 (tensor(4.7448, grad_fn=<AddBackward0>), tensor(2.3871, grad_fn=<AddBackward0>), tensor(2.3577, grad_fn=<AddBackward0>))\n",
      "8 (tensor(4.5211, grad_fn=<AddBackward0>), tensor(2.2980, grad_fn=<AddBackward0>), tensor(2.2231, grad_fn=<AddBackward0>))\n",
      "9 (tensor(4.3310, grad_fn=<AddBackward0>), tensor(2.2137, grad_fn=<AddBackward0>), tensor(2.1173, grad_fn=<AddBackward0>))\n",
      "10 (tensor(4.1429, grad_fn=<AddBackward0>), tensor(2.1338, grad_fn=<AddBackward0>), tensor(2.0091, grad_fn=<AddBackward0>))\n",
      "11 (tensor(3.9784, grad_fn=<AddBackward0>), tensor(2.0642, grad_fn=<AddBackward0>), tensor(1.9142, grad_fn=<AddBackward0>))\n",
      "12 (tensor(3.8294, grad_fn=<AddBackward0>), tensor(1.9856, grad_fn=<AddBackward0>), tensor(1.8438, grad_fn=<AddBackward0>))\n",
      "13 (tensor(3.6882, grad_fn=<AddBackward0>), tensor(1.9247, grad_fn=<AddBackward0>), tensor(1.7636, grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "losses=[]\n",
    "for epoch in range(CONF[\"epoch_num\"]):\n",
    "    #loss,row_loss,col_loss = train(model,optimizer,x,row_edges_index,col_edges_index)\n",
    "    loss = train(model,optimizer,x,row_edges_index,col_edges_index)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    #print('Epoch: {:03d}, Row Loss{:.4f} Col Loss: {:.4f}, Loss: {:.4f}'.format(epoch,row_loss,col_loss,loss))\n",
    "    print(epoch,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Extract the latent cell vectors, generate table vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_vectors(model,x,row_edges_index,col_edges_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, row_edges_index,col_edges_index)\n",
    "        cell_vectors = z.numpy()\n",
    "    return z,cell_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z,cell_vectors = get_cell_vectors(model,x,row_edges_index,col_edges_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list=generate_table_vectors(cell_vectors,tokenized_tables,s_vocab=shuffled_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score=evaluate_model(dataset,vec_list,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "table2vec",
   "language": "python",
   "name": "table2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
